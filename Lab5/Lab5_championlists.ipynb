{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wJg4uNhr5RQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e87VdoW0r59Z",
        "outputId": "d23df1aa-f721-4333-d8d8-0ed2deb0e548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents=[\n",
        "    \"This is document 1. It contains some text for testing.\",\n",
        "    \"Document 2 has different content for testing purposes.\",\n",
        "    \"This is a test query for information retrieval\"\n",
        "\n",
        "]\n",
        "query=\"This is a test query for information retrieval.\"\n",
        "\n",
        "#Tokenize the query and documens\n",
        "query_tokens=word_tokenize(query.lower())\n",
        "documents_tokens=[word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "\n",
        "documents_tokens\n",
        "\n",
        "#Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer=TfidfVectorizer()\n",
        "\n",
        "#calculate TF-IDF scores for documents and query\n",
        "tfidf_matrix=tfidf_vectorizer.fit_transform([\" \".join(doc) for doc in documents_tokens])\n",
        "query_tfidf=tfidf_vectorizer.transform([\" \".join(query_tokens)])\n",
        "\n",
        "\n",
        "# print(query_tokens)\n",
        "\n",
        "#calculate the cosine similarity between query and documents\n",
        "cosine_similarities=np.dot(query_tfidf,tfidf_matrix.T).toarray()[0]\n",
        "\n",
        "#create a champion list for each query term\n",
        "champion_lists={}\n",
        "\n",
        "for i,term in enumerate(query_tokens):\n",
        "  term_documents=tfidf_matrix[:,i].toarray().flatten()\n",
        "  champion_list=np.argsort(term_documents)[::-1][:3]   #adjust the number of champions as needed\n",
        "  champion_lists[term]=champion_list\n",
        "\n",
        "\n",
        "print(\"Champion lists:\\n\",champion_lists)\n",
        "\n",
        "#initialized a set to store the selected documents\n",
        "selected_documents=set()\n",
        "\n",
        "#Add documents from champion lists to the selected set\n",
        "for term in query_tokens:\n",
        "  selected_documents.update(champion_lists.get(term,[]))\n",
        "\n",
        "# #Sort the selected docuemnts by cosine similarity\n",
        "selected_documents=list(selected_documents)\n",
        "selected_documents.sort(key=lambda idx: -cosine_similarities[idx])\n",
        "\n",
        "print(\"\\n\")\n",
        "#Print ranked documents\n",
        "print(\"ranked Documents:\")\n",
        "for idx in selected_documents:\n",
        "  print(f\"Document {idx + 1}:{documents[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMEd7Fz1aPG",
        "outputId": "2feda73b-ee3e-4195-81cd-bef5613b888c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Champion lists:\n",
            " {'this': array([0, 2, 1]), 'is': array([1, 2, 0]), 'a': array([1, 2, 0]), 'test': array([1, 0, 2]), 'query': array([2, 1, 0]), 'for': array([1, 2, 0]), 'information': array([2, 1, 0]), 'retrieval': array([2, 0, 1]), '.': array([0, 2, 1])}\n",
            "\n",
            "\n",
            "ranked Documents:\n",
            "Document 3:This is a test query for information retrieval\n",
            "Document 1:This is document 1. It contains some text for testing.\n",
            "Document 2:Document 2 has different content for testing purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3UmLS511bj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}