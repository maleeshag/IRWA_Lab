{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4s2qhXgOYct"
      },
      "outputs": [],
      "source": [
        "###Question 03 ###\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import re   #regular expressions - regettes\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os"
      ],
      "metadata": {
        "id": "MiukxjkQQLXn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05dZGJKRBmg",
        "outputId": "dad503c3-c772-4ab7-ed92-5c96da270e47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0jy5fJLRScu",
        "outputId": "9d17f7f0-b38f-4ef1-8a9d-b1cd4c8876c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/IRWA_lab/positional/"
      ],
      "metadata": {
        "id": "prYF3nJgRbZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcefde0e-c3fe-48b8-adf1-78b31f033427"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IRWA_lab/positional\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary=dict()\n",
        "documents=dict()\n",
        "for doc_no in range(1,4):\n",
        "  with open (os.getcwd()+ '/doc_'+str(doc_no)+\".txt\",'r') as file:\n",
        "     s=file.read().replace('\\n',' ')\n",
        "\n",
        "  #normalizing\n",
        "  #replacing the unnessary characters( won't - > will not )\n",
        "  s=re.sub(' ',' ',s)\n",
        "  s=re.sub(r\"won\\'t\",\"will not\", s)\n",
        "  s=re.sub(r\"can't\",\"can not\", s)\n",
        "  s=re.sub(r\"n\\'t\",\" not\", s)\n",
        "  s=re.sub(r\"\\'re\",\" are\", s)\n",
        "  s=re.sub(r\"\\'s\",\" is\", s)\n",
        "  s=re.sub(r\"\\'d\",\" would\", s)\n",
        "  s=re.sub(r\"\\'ll\",\" will\", s)\n",
        "  s=re.sub(r\"\\'t\",\" not\", s)\n",
        "  s=re.sub(r\"\\'ve\",\" have\", s)\n",
        "  s=re.sub(r\"\\'m\",\" am\", s)\n",
        "  s=re.sub(r'[^\\w\\s]','', s)   #remove white spaces\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "v10rD9wmgtLv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  stop_words = stopwords.words('english')\n",
        "  words = word_tokenize(str(s))"
      ],
      "metadata": {
        "id": "x_7JO4A-guzx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [word for word in words if word.lower() not in stop_words]\n",
        "key = \"doc_\" + str(doc_no)"
      ],
      "metadata": {
        "id": "ohv8kBEeg3Ht"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "stemmed = []\n",
        "\n",
        "# stemming\n",
        "for i in new_text:\n",
        "  # print(i)\n",
        "  stemmed.append(ps.stem(i))\n",
        "\n",
        "# creating positional index posting list\n",
        "temp_dict = dict()\n",
        "a = 0\n",
        "\n",
        "for x in stemmed:\n",
        "  # assign numbers to each word to identify the position\n",
        "  key = x\n",
        "  temp_dict.setdefault(key, [])\n",
        "  temp_dict[key].append(a)\n",
        "  a += 1\n",
        "\n",
        "for x in temp_dict:\n",
        "  # check whether the term already in the dictionary\n",
        "  if x in dictionary:\n",
        "    dictionary[x][doc_no] = temp_dict.get(x)\n",
        "\n",
        "  else:\n",
        "    # if not add a new key assigning the term to the dictionary\n",
        "    key = x\n",
        "    dictionary.setdefault(key, [])\n",
        "    dictionary[key] = {}\n",
        "    dictionary[x][doc_no] = temp_dict.get(x)"
      ],
      "metadata": {
        "id": "zYscvGwsg7QN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOiIg7gshCXr",
        "outputId": "6df28041-3471-4465-f56e-7d02528864ef"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'remark': [0], 'foreign': [1, 21, 37, 54, 122, 136, 149], 'polici': [2, 22, 38, 55, 123, 137, 150], 'nation': [3, 11], 'press': [4], 'club': [5], 'washingtonthank': [6], 'opportun': [7], 'speak': [8], 'thank': [9], 'center': [10], 'interest': [12, 58, 183], 'honor': [13], 'invit': [14, 40], 'would': [15], 'like': [16], 'talk': [17], 'today': [18, 48], 'develop': [19, 128], 'new': [20, 41, 43, 129, 131], 'direct': [23, 46], 'countri': [24, 181], 'one': [25, 148], 'replac': [26, 144], 'random': [27], 'purpos': [28], 'ideolog': [29], 'strategi': [30], 'chao': [31, 167], 'peac': [32], 'time': [33, 39, 96, 132, 134], 'shake': [34], 'rust': [35], 'america': [36, 68], 'voic': [42], 'vision': [44, 130], 'fold': [45], 'outlin': [47], 'also': [49], 'return': [50], 'us': [51], 'timeless': [52], 'principl': [53], 'alway': [56], 'put': [57], 'american': [59, 61], 'peopl': [60], 'secur': [62], 'els': [63], 'foundat': [64], 'everi': [65], 'decis': [66], 'make': [67, 139, 178], 'first': [69, 78], 'major': [70], 'overrid': [71], 'theme': [72], 'administr': [73], 'chart': [74], 'path': [75], 'forward': [76], 'must': [77], 'briefli': [79], 'look': [80], 'back': [81, 90], 'lot': [82], 'proud': [83], '1940': [84], 'save': [85, 94], 'world': [86, 95], 'greatest': [87], 'gener': [88], 'beat': [89], 'nazi': [91], 'japanes': [92], 'imperialist': [93], 'totalitarian': [97], 'commun': [98], 'cold': [99, 120], 'war': [100, 121], 'last': [101], 'decad': [102], 'democrat': [103], 'republican': [104], 'work': [105], 'togeth': [106], 'got': [107], 'mr': [108], 'gorbachev': [109], 'heed': [110], 'word': [111], 'presid': [112, 158], 'reagan': [113], 'said': [114], 'tear': [115], 'wall': [116], 'histori': [117], 'forget': [118], 'unfortun': [119], 'veer': [124], 'badli': [125], 'cours': [126], 'fail': [127], 'fact': [133], 'went': [135, 153], 'began': [138, 174], 'less': [140, 141], 'sens': [142], 'logic': [143], 'foolish': [145], 'arrog': [146], 'led': [147], 'disast': [151], 'anoth': [152], 'mistak': [154], 'iraq': [155], 'egypt': [156], 'libya': [157], 'obama': [159], 'line': [160], 'sand': [161], 'syria': [162], 'action': [163], 'help': [164], 'throw': [165], 'region': [166], 'gave': [168], 'isi': [169], 'space': [170], 'need': [171], 'grow': [172], 'prosper': [173], 'danger': [175], 'idea': [176], 'could': [177], 'western': [179, 185], 'democraci': [180, 186], 'experi': [182], 'becom': [184], 'tore': [187], 'institut': [188], 'surpris': [189], 'unleash': [190]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udTxssAThEdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}